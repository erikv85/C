\documentclass[a4paper, 11pt]{article}

\usepackage{amsmath}

\title{HPP Re-Exam Report}
\author{Erik Vesterlund}

\begin{document}
\maketitle

\section{HW, Compiler and Initial Results}

All experiments were run under Linux Ubuntu-14.04 (64 bit) on an Intel Xeon~E5520 of 2.27~Ghz with 8~MB L3 cache and a 24~GB RAM. To measure time we used the \texttt{real} output of \texttt{time}\footnote{\texttt{\$ time ./app args}}, and for compilation we used GCC version 4.8.4. Our baseline results are shown in table \ref{table:baseline} (output formats had been modified prior to running).  Variable and function names are written in camel case from now, where applicable, for ease of writing.

\begin{table}[!h]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Value} & \textbf{Case A}       & \textbf{Case B}        \\ \hline
oneNormOfH              & 40014.062500 & 266690.104167 \\ \hline
eigValApproxShiftedBack & 2.446041     & 2.883391      \\ \hline
energyAnalytical        & 2.500000     & 2.500000      \\ \hline
waveFunctionSquaredSum  & 1.000000     & 1.000000      \\ \hline
waveFunctionSquaredMax  & 2.610440     & 2.821460      \\ \hline
maxRelDiff              & 0.0251857    & 0.224523      \\ \hline
energyDiff              & -0.053959188 & 0.383391178   \\ \hline
Time                    & 9.197        & 32.195        \\ \hline
\end{tabular}
\caption{Baseline results}
\label{table:baseline}
\end{table}

\section{Improvements}

%\subsection{Row-major ordering}

\subsection{Profiling}

The first improvement made was to change the matrix-vector multiplication from calculating in a column-major fashion to row-major, which gave the results
$$8.302, 22.844$$

This improvement is due to memory layout being row-major, so changing our algorithm to that increases spatial locality.

We introduced timers for the calls to \texttt{getHmatrixElement}, \texttt{getOneNorm}, \texttt{getPotentialValue} and \texttt{doPowerMethod}. For both case A and B, the call to \texttt{doPowerMethod} accounted for nearly $100\%$ of computation time.

The function \texttt{doPowerMethod} makes $M$ calls to an $O(N^2)$ matrix-vector multiplication function (the most expensive function), whence \texttt{doPowerMethod} is $O(MN^2)$.

\subsection{Parallel Matrix-Vector Multiplication}

Our first idea was to fuse matrix-vector multiplication and dot product calculations (where possible), and then parallellize that. We created a thread-data type, which contains (among other things) a matrix $A$, two vectors $x,y$ and two scalar pointers. Then we implemented the \texttt{parallelMatVecMul} function, which computes $y \leftarrow Ax$, $d_1 \leftarrow x\bullet y$ and $d_2 \leftarrow x\bullet x$; the $d_i$ are shared among threads whence those updates form a critical section, which is handled with mutex locks.

The function \texttt{parallelMatVecMul} is still $O(N^2)$, and if the number of threads doesn't divide the dimension then the "last" thread does the remainder. In \texttt{powermethod.c}, we have added the function \texttt{parallelPm} to account for threading. If the number of threads supplied by the user is only one, then the program defaults to using \texttt{doPowerMethod}.

As can be seen in table \ref{table:parall}, improvements top out at eight threads, adding more threads than that increases time. This might be due to thread-creation overhead or the poor load balancing described above.

\begin{table}[!h]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Threads} & \textbf{Case A} & \textbf{Case B} \\ \hline
4       & 6.297  & 8.388  \\ \hline
8       & 4.804  & 7.147  \\ \hline
12      & 5.004  & 9.764  \\ \hline
16      & 5.853  & 9.860  \\ \hline
\end{tabular}
\caption{Parallelization results}
\label{table:parall}
\end{table}

\subsection{Customized Matrix-Vector Multiplication}

We then looked at the matrix itself with the goal to write a customized matrix-vector multiplication function with parallelization. The matrices generated by the program are sparse, and in particular we have (using $0$-indexation):

$$(Ax)_i = \begin{cases}
A_{00}x_0 + A_{01}x_1+A_{0,N-1}x_{N-1} & \mbox{if } i=0 \\
A_{N-1,0} x_0 + A_{N-1,N-2} x_{N-2} + A_{N-1,N-1} x_{N-1} & \mbox{if } i=N-1 \\
A_{i,i-1} x_{i-1} + A_{ii} x_i + A_{i,i+1} x_{i+1} & \mbox{otherwise}
\end{cases}$$

Using these simple formulas we can reduce the complexity of the power iterations to $O(MN)$, and the results can be seen in table \ref{table:parall-custom}. With those times other factors start becoming relevant, such as \texttt{getHmatrixElement}. Again the results get worse with "too many" threads, due to the factors mentioned before. Thread-creation overhead likely has a greater impact here since the overall computation time is drastically lower.

\begin{table}[!h]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Threads }& \textbf{Case A} & \textbf{Case B} \\ \hline
4       & 0.678  & 0.284  \\ \hline
8       & 1.973  & 0.637  \\ \hline
12      & 3.358  & 0.898  \\ \hline
16      & 4.839  & 1.251  \\ \hline
\end{tabular}
\caption{Parallel-custom results}
\label{table:parall-custom}
\end{table}

\subsection{Compiler Optimization}

Finally we tried optimizing with \texttt{O3} but that didn't give any noticeable results either with or without threading, as seen in table \ref{table:parall-custom-optim}.

\begin{table}[!h]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Threads} & \textbf{Case A} & \textbf{Case B} \\ \hline
1       & 8.209  & 22.756 \\ \hline
4       & 0.678  & 0.277  \\ \hline
8       & 2.011  & 0.642  \\ \hline
12      & 3.453  & 0.907  \\ \hline
16      & 5.026  & 1.182  \\ \hline
\end{tabular}
\caption{Parallel-custom with compiler optimization}
\label{table:parall-custom-optim}
\end{table}

\section{Unexplored or Unsuccessful}

I had done matrix-vector multiplication using SIMD instructions in another class, with success, but I couldn't get it to work here (wrong results) and I have no idea why (despite the only difference being float/double and loop stride), the code can be seen in \texttt{linalg.c}. SIMD instructions could have been used together with all the above functions, as well as in other functions.

Finally, the whole power method might be parallelized, but then there are more dependencies which would require more synchronization, creating overhead.

\end{document}






















